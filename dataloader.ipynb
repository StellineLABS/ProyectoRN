{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa77cc96-f623-4a27-bd3c-b30a3c0513f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02fabc0-fb89-47e5-b04d-6ccb61839e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset CSV\n",
    "df = pd.read_csv('samples.csv')\n",
    "labels = [*set(list(df['sample_track_id']))]\n",
    "d = {'index': list(range(0, len(labels))), 'mid': labels, 'display_name':[\"''\"]*len(labels)}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('class_labels_indices.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b029b3-2ff8-40f8-a96a-500c0055cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON\n",
    "directory = 'canciones'\n",
    "df = pd.read_csv('samples.csv')\n",
    "diccionarios = []\n",
    "# Iteramos sobre archivos en ./canciones\n",
    "for filename in os.listdir(directory):\n",
    "    direccion = os.path.join(directory, filename)\n",
    "    if os.path.isfile(direccion):\n",
    "        # Quitamos extensión\n",
    "        original = filename.replace(\".flac\", \"\")\n",
    "        # Determinamos qué samples contiene cada canción según samples.csv\n",
    "        etiquetas = ','.join([*set([str(df['original_track_id'][i]) for i in list(df.index[df['sample_track_id'] == original])])])\n",
    "        # Placeholder si una canción no contiene samples\n",
    "        if etiquetas == '':\n",
    "            etiquetas = 'T000'\n",
    "        diccionario = {\n",
    "        \"wav\": direccion,\n",
    "        \"labels\": etiquetas\n",
    "        }\n",
    "        diccionarios.append(diccionario)\n",
    "        \n",
    "data = {\n",
    "    \"data\":diccionarios\n",
    "}\n",
    "json_object = json.dumps(data, indent=4)\n",
    "# Creamos json del dataset\n",
    "with open(\"train_data.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2aee5a-d8c3-4328-98e9-46133f7454e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_dict(label_csv):\n",
    "    index_lookup = {}\n",
    "    with open(label_csv, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            index_lookup[row['mid']] = row['index']\n",
    "            line_count += 1\n",
    "    return index_lookup\n",
    "\n",
    "def preemphasis(signal,coeff=0.97):\n",
    "    \"\"\"perform preemphasis on the input signal.\n",
    "\n",
    "    :param signal: The signal to filter.\n",
    "    :param coeff: The preemphasis coefficient. 0 is none, default 0.97.\n",
    "    :returns: the filtered signal.\n",
    "    \"\"\"\n",
    "    return np.append(signal[0],signal[1:]-coeff*signal[:-1])\n",
    "\n",
    "class AudiosetDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None):\n",
    "        \"\"\"\n",
    "        Dataset that manages audio recordings\n",
    "        :param audio_conf: Dictionary containing the audio loading and preprocessing settings\n",
    "        :param dataset_json_file\n",
    "        \"\"\"\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "\n",
    "        self.data = data_json['data']\n",
    "        self.audio_conf = audio_conf = {'num_mel_bins': 128, 'target_length': 1024, 'freqm': 24, 'timem': 192, 'mixup': 0.5}\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.index_dict = make_index_dict(label_csv)\n",
    "        self.label_num = len(self.index_dict)\n",
    "        print('number of classes is {:d}'.format(self.label_num))\n",
    "        \n",
    "    def _wav2fbank(self, filename):\n",
    "        # mixup\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "    \n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        target_length = self.audio_conf.get('target_length')\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns: image, audio, nframes\n",
    "        where image is a FloatTensor of size (3, H, W)\n",
    "        audio is a FloatTensor of size (N_freq, N_frames) for spectrogram, or (N_frames) for waveform\n",
    "        nframes is an integer\n",
    "        \"\"\"        \n",
    "        datum = self.data[index]\n",
    "        label_indices = np.zeros(self.label_num)\n",
    "        fbank = self._wav2fbank(datum['wav'])\n",
    "        for label_str in datum['labels'].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] = 1.0\n",
    "        label_indices = torch.FloatTensor(label_indices)\n",
    "        fbank = torch.transpose(fbank, 0, 1)\n",
    "        # this is just to satisfy new torchaudio version, which only accept [1, freq, time]\n",
    "        fbank = fbank.unsqueeze(0)\n",
    "        # squeeze it back, it is just a trick to satisfy new torchaudio version\n",
    "        fbank = fbank.squeeze(0)\n",
    "        fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "        # the output fbank shape is [time_frame_num, frequency_bins], e.g., [1024, 128]\n",
    "        return fbank, label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3f42f-9770-477b-9046-adcdc395d92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
