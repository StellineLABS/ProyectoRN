{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06984777-9267-40dc-8977-e98bab83782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "def make_index_dict(label_csv):\n",
    "    index_lookup = {}\n",
    "    with open(label_csv, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            index_lookup[row['mid']] = row['index']\n",
    "            line_count += 1\n",
    "    return index_lookup\n",
    "\n",
    "def make_name_dict(label_csv):\n",
    "    name_lookup = {}\n",
    "    with open(label_csv, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            name_lookup[row['index']] = row['display_name']\n",
    "            line_count += 1\n",
    "    return name_lookup\n",
    "\n",
    "def lookup_list(index_list, label_csv):\n",
    "    label_list = []\n",
    "    table = make_name_dict(label_csv)\n",
    "    for item in index_list:\n",
    "        label_list.append(table[item])\n",
    "    return label_list\n",
    "\n",
    "def preemphasis(signal,coeff=0.97):\n",
    "    \"\"\"perform preemphasis on the input signal.\n",
    "\n",
    "    :param signal: The signal to filter.\n",
    "    :param coeff: The preemphasis coefficient. 0 is none, default 0.97.\n",
    "    :returns: the filtered signal.\n",
    "    \"\"\"\n",
    "    return np.append(signal[0],signal[1:]-coeff*signal[:-1])\n",
    "\n",
    "class AudiosetDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, audio_conf, label_csv=None):\n",
    "        \"\"\"\n",
    "        Dataset that manages audio recordings\n",
    "        :param audio_conf: Dictionary containing the audio loading and preprocessing settings\n",
    "        :param dataset_json_file\n",
    "        \"\"\"\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "\n",
    "        self.data = data_json['data']\n",
    "        self.audio_conf = audio_conf\n",
    "        print('---------------the {:s} dataloader---------------'.format(self.audio_conf.get('mode')))\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.freqm = self.audio_conf.get('freqm')\n",
    "        self.timem = self.audio_conf.get('timem')\n",
    "        print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        self.dataset = self.audio_conf.get('dataset')\n",
    "        print('now process ' + self.dataset)\n",
    "        # dataset spectrogram mean and std, used to normalize the input\n",
    "        self.norm_mean = self.audio_conf.get('mean')\n",
    "        self.norm_std = self.audio_conf.get('std')\n",
    "        # skip_norm is a flag that if you want to skip normalization to compute the normalization stats using src/get_norm_stats.py, if Ture, input normalization will be skipped for correctly calculating the stats.\n",
    "        # set it as True ONLY when you are getting the normalization stats.\n",
    "        self.skip_norm = self.audio_conf.get('skip_norm') if self.audio_conf.get('skip_norm') else False\n",
    "        if self.skip_norm:\n",
    "            print('now skip normalization (use it ONLY when you are computing the normalization stats).')\n",
    "        else:\n",
    "            print('use dataset mean {:.3f} and std {:.3f} to normalize the input.'.format(self.norm_mean, self.norm_std))\n",
    "        # if add noise for data augmentation\n",
    "        self.noise = self.audio_conf.get('noise')\n",
    "        if self.noise == True:\n",
    "            print('now use noise augmentation')\n",
    "\n",
    "        self.index_dict = make_index_dict(label_csv)\n",
    "        self.label_num = len(self.index_dict)\n",
    "        print('number of classes is {:d}'.format(self.label_num))\n",
    "        \n",
    "    def _wav2fbank(self, filename):\n",
    "        # mixup\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "    \n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "        target_length = self.audio_conf.get('target_length')\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "        return fbank\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns: image, audio, nframes\n",
    "        where image is a FloatTensor of size (3, H, W)\n",
    "        audio is a FloatTensor of size (N_freq, N_frames) for spectrogram, or (N_frames) for waveform\n",
    "        nframes is an integer\n",
    "        \"\"\"        \n",
    "        datum = self.data[index]\n",
    "        label_indices = np.zeros(self.label_num)\n",
    "        fbank = self._wav2fbank(datum['wav'])\n",
    "        for label_str in datum['labels'].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] = 1.0\n",
    "        label_indices = torch.FloatTensor(label_indices)\n",
    "\n",
    "        # SpecAug, not do for eval set\n",
    "        freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "        timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "        fbank = torch.transpose(fbank, 0, 1)\n",
    "        # this is just to satisfy new torchaudio version, which only accept [1, freq, time]\n",
    "        fbank = fbank.unsqueeze(0)\n",
    "        if self.freqm != 0:\n",
    "            fbank = freqm(fbank)\n",
    "        if self.timem != 0:\n",
    "            fbank = timem(fbank)\n",
    "        # squeeze it back, it is just a trick to satisfy new torchaudio version\n",
    "        fbank = fbank.squeeze(0)\n",
    "        fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "        # normalize the input for both training and test\n",
    "        if not self.skip_norm:\n",
    "            fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
    "        # skip normalization the input if you are trying to get the normalization stats.\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.noise == True:\n",
    "            fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "            fbank = torch.roll(fbank, np.random.randint(-10, 10), 0)\n",
    "\n",
    "        # the output fbank shape is [time_frame_num, frequency_bins], e.g., [1024, 128]\n",
    "        return fbank, label_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
