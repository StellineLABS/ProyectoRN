{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51238e7f-1e42-4497-89b9-702d5feef3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar dependencias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04dc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# override the timm package to relax the input shape constraint.\n",
    "class PatchEmbed(nn.Module):\n",
    "    '''\n",
    "    Embeding(?)\n",
    "    Parametros: img_size = dimension del input \n",
    "    '''\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f0d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, \n",
    "                 norm_layer=None, bias=True,drop=0.,use_conv=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n",
    "\n",
    "        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n",
    "        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f491557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_norm=qk_norm,attn_drop=attn_drop,proj_drop=proj_drop, norm_layer=norm_layer)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c3371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/britnybrito/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: numero de clases. the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: division del patch en dim frecuencia. the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: division del patch en dim tiempo. the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: bins de frecuencia en entrada. the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: frames de tiempo en entrada. the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: bool imageNet pretrain. if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: bool audio e image pretrain. if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: dimensiones del ast. the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True,\n",
    "                 audioset_pretrain=False, model_size='base384'):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "        #QT\n",
    "        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
    "        \n",
    "        # Modelo base  model_size == 'small224':\n",
    "                #self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "        self.patch_embed = PatchEmbed()\n",
    "        self.original_num_patches = self.patch_embed.num_patches\n",
    "        self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "        \n",
    "        embed_len = self.patch_embed.num_patches #QT(?)if no_embed_class else num_patches + self.num_prefix_tokens \n",
    "        self.embed_dim = self.patch_embed.embed_dim #(?)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.original_num_patches + 2, self.embed_dim))\n",
    "                        #QT nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
    "        \n",
    "        \n",
    "        self.original_embedding_dim = self.pos_embed.shape[2]\n",
    "        #cabezas\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), \n",
    "                                          nn.Linear(self.original_embedding_dim, label_dim))\n",
    "        # automatcially get the intermediate shape\n",
    "        f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "        num_patches = f_dim * t_dim\n",
    "        self.patch_embed.num_patches = num_patches\n",
    "        # the linear projection layer\n",
    "        new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        self.patch_embed.proj = new_proj\n",
    "        # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "        # TODO can use sinusoidal positional embedding instead\n",
    "        new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "        self.pos_embed = new_pos_embed#bien doble definici√≥n\n",
    "        trunc_normal_(self.v.pos_embed, std=.02)\n",
    "        #self.cls_token = tf.Variable(initial_value=initial_value, trainable=True, name=\"cls\")\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))# if class_token else None #(?)\n",
    "        # https://huggingface.co/spaces/Hila/RobustViT/blob/main/ViT/ViT_new.py\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "                        #QT nn.Parameter(torch.zeros(1, 1, embed_dim)) #if distilled else \n",
    "        pos_drop_rate = 0.0\n",
    "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
    "        num_heads=6\n",
    "        mlp_ratio=4\n",
    "        qkv_bias=True\n",
    "        #qk_norm=False #pred\n",
    "        #init_values=\n",
    "        #proj_drop=\n",
    "        #attn_drop=\n",
    "        #drop_path=\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "        #act_layer=\n",
    "        #mlp_layer=\n",
    "        #block_fn: Callable = Block,\n",
    "         #QT mover a embeded(?)patch_size=16, embed_dim=384\n",
    "        \n",
    "        depth = 12\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=self.embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                #qk_norm=qk_norm,\n",
    "                #init_values=init_values,\n",
    "                #proj_drop=proj_drop_rate,\n",
    "                #attn_drop=attn_drop_rate,\n",
    "                #drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                #act_layer=act_layer,\n",
    "                #mlp_layer=mlp_layer,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)#norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x) #.forward(x)(?)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        #x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)#alternativa\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x) \n",
    "        for blk in self.blocks:#aaaa\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6059f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
